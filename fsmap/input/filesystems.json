[
  {
    "year": "2002",
    "filesystem": "Lustre",
    "description": "HPC filesystem",
    "techniques": [
      ["Lock Management", "Distributed across cluster", "authority manages parts of the namespace and responds to client requests for locks"],
      ["Caching Inodes", "Metadata requests", "caches the results of lookup() metadata requests"],
      ["Relax Consistency", "Batching requests", "enabled with a configuration parameter because the weakened metadata consistency may break some applications."]
    ]
  },
  {
    "year": "2002",
    "filesystem": "GPFS",
    "description": "General Purpose File System",
    "techniques": [
      ["Lock Management", "Distributed across cluster", "servers dynamically granted authority access to inodes (e.g., guard locks, synchronize access)"],
      ["Caching Inodes", "Metaata requests", "caches the results of stat() metadata requests"]
    ]
  },
  {
    "year": "2003",
    "filesystem": "GFS",
    "description": "Google File System",
    "techniques": [
      ["Lock Management", "Single node metadata service", "simple lock configurations like timeouts"],
      ["Caching Inodes", "Part of the inode", "caches file-system specific metadata, such as the location/striping strategies"],
      ["Relax Consistency", "Deviate from POSIX IO", "file state is undefined (not consistent), so applications use append() instead of write()/seek()"],
      ["Journal Safety", "Global durability", "updates are always safe because the master replicates its journal to remote nodes"]
    ]
  },
  {
    "year": "2003",
    "filesystem": "LH",
    "description": "Lazy Hybrid",
    "techniques": [
      ["Caching Paths", "Parent metadata", "maintains separate per-file metadata just for permissions"]
    ]
  },
  {
    "year": "2004",
    "filesystem": "CephFS",
    "description": "Ceph File System",
    "techniques": [
      ["Lock Management", "Distributed across cluster", "authority manages parts of the namespace and responds to client requests for locks"],
      ["Caching Inodes", "Entire inodes", "client/server cache whole inode to reduce the numbe of RPCs"],
      ["Relax Consistency", "Configurable POSIX IO deviation", "Lazy IO option lets clients buffer reads/writes of open files using their own cache-coherency mechanisms"],
      ["Journal Format", "Custom metadata format", "events are materialized in memory and streamed into the object store; when they land on disk they have a custom layout"],
      ["Journal Safety", "Global durability", "updates are always safe because the metadata cluster streams the updates into the object store"],
      ["Metadata Distribution", "Subtree partitioning", "carve namespace into subtrees, locate them with pointers in boundary inodes"],
      ["Load Balancing", "Partition on writes, replicate on reads", "Hashes directories on writes at certain size or temperature; replicates inodes on reads at a certain read temperature"]
    ]
  },
  {
    "year": "2005",
    "filesystem": "PVFS2",
    "description": "Parallel Virtual File System",
    "techniques": [
      ["Lock Management", "Distributed across cluster", "spins up metadata servers on both storage and non-storage servers"],
      ["Caching Inodes", "Part of the inode", "separates metadata into a namespace cache and an attribute cache"],
      ["Relax Consistency", "Batching requests", "combines create() requests together, which creates windows of metadata inconsistency."]
    ]
  },
  {
    "year": "2005",
    "filesystem": "pNFS",
    "description": "Parallel NFS",
    "techniques": [
      ["Caching Inodes", "Entire inodes", "with Panache, metadata is cached on a multi-node cluster backed by a remote site"],
      ["Metadata Distribution", "Table-based mapping", "clients store maps of pathnames to servers"]
    ]
  },
  {
    "year": "2005",
    "filesystem": "HDFS",
    "description": "Hadoop Distributed File System",
    "techniques": [
      ["Lock Management", "Single node metadata service", "simple lock configurations like timeouts"],
      ["Relax Consistency", "Deviate from POSIX IO", "lets client read stale buffers of open files"],
      ["Metadata Distribution", "Subtree partitioning", "HDFS Federation supports statically assigning metadata to servers"]
    ]
  },
  {
    "year": "2006",
    "filesystem": "SkyFS",
    "description": "SkyFS",
    "techniques": [
      ["Caching Paths", "Clients cache metadata location", "FIXME"]
    ]
  },
  {
    "year": "2006",
    "filesystem": "Farsite",
    "description": "Farsite",
    "techniques": [
      ["Metadata Distribution", "Subtree Partitioning", "statically assigns metadata to servers at boot time, co-locating related inode ranges (e.g., inodes in the same subtree)"]
    ]
  },
  {
    "year": "2008",
    "filesystem": "PanFS",
    "description": "Panasas File System",
    "techniques": [
      ["Lock Management", "Distributed across cluster", "authority manages parts of the namespace and responds to client requests for locks"],
      ["Caching Inodes", "Directories and Metadata requests", "clients cache directories for reading and parsing/caches the result of metadata stat() requests"],
      ["Relax Consistency", "Batching requests", "Packages similar requests (e.g., create(), state) into one message, which creates windows of metadata inconsistency, directories replicated on RAID1"],
      ["Journal Format", "Requests split into separate logs", "requests partioned in software to different logs (e.g., op-logs, cap-logs) because of differing request overheads"],
      ["Journal Safety", "Global durability", "journal stored on battery-backed NVRAM and replicated to peers and corresponding metadata stored on objects"],
      ["Metadata Distribution", "Subtree partitioning", "statically assigns metadata to servers"]
    ]
  },
  {
    "year": "2009",
    "filesystem": "IBRIX",
    "description": "IBRIX File System",
    "techniques": []
  },
  {
    "year": "2008",
    "filesystem": "HBA",
    "description": "Hierarchical Bloom Analysis",
    "techniques": [
      ["Caching Paths", "Clients cache metadata location", "hierarchical bloom filter arrays probabilistically identify metadata servers"]
    ]
  },
  {
    "year": "2010",
    "filesystem": "Ursa Minor",
    "description": "Ursa Minor File System",
    "techniques": [
      ["Caching Inodes", "Part of the inode", "reduce overheads of NFS by storing version history in memory on storage nodes"],
      ["Metadata Distribution", "Subtree Partitioning", "statically assigns metadata to servers at boot time, co-locating related inode ranges (e.g., inodes in the same subtree)"]
    ]
  },
  {
    "year": "2012",
    "filesystem": "GlusterFS",
    "description": "Gluster File System",
    "techniques": [
      ["Lock Management", "Distributed across cluster", "metadata stored with data, so object stores holding the data brick are the authorities for that metadata"],
      ["Metadata Distribution", "Hashed across cluster", "operations isolated on each server and load balanced evenly across the cluster"]
    ]
  },
  {
    "year": "2014",
    "filesystem": "GIGA+/IndexFS",
    "description": "Includes TableFS",
    "techniques": [
      ["Lock Management", "Distributed across cluster", "authority manages parts of the namespace and responds to client requests for locks"],
      ["Caching Inodes", "Part of the inode", "caches permission metadata (ACLs)"],
      ["Journal Format", "LSM Tree metadata format", "pioneered by TableFS, size/quantity of file system metadata fits this format and existing implementations are abundant"],
      ["Caching Paths", "Parent metadata", "clients cache permissions to improve lookups/creates when processing GIGA+'s partition history traveral; IndexFS only caches permissions (helps lookups/creates)"],
      ["Metadata Distribution", "Hash inodes/directories", "inodes and large directories are hashed (for locality); subtree partitioning is used to describe higher path components (ancestors) being round robined across the cluster for balance"],
      ["Load Balancing", "Hash directories", "when number of directory entries reaches a threshold, hash inodes across multiple servers, saving locations in parent inode; writes are fast but reads are subject to cache locality and lease expirations"]
    ]
  },
  {
    "year": "2014",
    "filesystem": "Batch/DeltaFS",
    "description": "",
    "techniques": [
      ["Relax Consistency", "Decouple namespaces", "Batch/DeltaFS lets clients pull metadata locally and merge updates eventually via client-funded metadata servers"],
      ["Journal Safety", "Local durability", "in Batch/DeltaFS, updates could be lost if the client dies and stays dead while holding unwritten updates"]
    ]
  },
  {
    "year": "2016",
    "filesystem": "ShardFS",
    "description": "CMU File System",
    "techniques": [
      ["Metadata Distribution", "Replicate inodes", "ancestor metadata replicated across servers; reads are fast but writes require locking and serialization because optimistic concurrency with a two-phase locking penalty is employed"],
      ["Load Balancing", "Replicate inodes", "Copies metadata immediately and pessimistically at creation time"]
    ]
  },
  {
    "year": "2015",
    "filesystem": "CalvinFS",
    "description": "Calvin File System",
    "techniques": [
      ["Metadata Distribution", "Table-based mapping", "clients store maps of pathnames to servers and the servers store metadata in relational databases"]
    ]
  },
  {
    "year": "2015",
    "filesystem": "MarFS",
    "description": "Mar File System",
    "techniques": [
      ["Relax Consistency", "Decouple namespace", "hot subtrees are managed by dedicated GPFS metadata clusters"]
    ]
  },
  {
    "year": "2015",
    "filesystem": "TwoTiers",
    "description": "EMC File System",
    "techniques": [
      ["Relax Consistency", "Decouple namespace", "hot subtrees are backed by a storage tier of SSDs"]
    ]
  },
  {
    "year": "2017",
    "filesystem": "ADLS",
    "description": "Azure Data Lake Store",
    "techniques": [
      ["Lock Management", "Centralizes most metadata", "... but some types of metadata (size, modification time, extents, etc.) stored with data"],
      ["Relax Consistency", "Deviate from POSIX IO", "non-POSIX IO APIs to facilitate fast execution of very specific applciations/services"]
    ]
  },
  {
    "year": "2017",
    "filesystem": "ColossusFS",
    "description": "Colossus File System",
    "techniques": [
      ["Metadata Distribution", "Table-based mapping", "using BigTable (distributed map), pathnames are mapped to inodes; inherits domain-specifc features lile aggressive caching"]
    ]
  },
  {
    "year": "2017",
    "filesystem": "HopsFS",
    "description": "Hops File System (fork of HDFS with more scalable metadata service",
    "techniques": [
      ["Relax Consistency", "Force path traversal order", "allows concurrent writers/readers of the same subtree"],
      ["Metadata Distribution", "Subtree partitioning", "configurable partitioner assigns inodes to servers; default policy is group siblings but hash children"]
    ]
  }
]
